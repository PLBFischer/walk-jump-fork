[instantiate_callbacks] Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[instantiate_callbacks] Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[instantiate_callbacks] Instantiating callback <lightning.pytorch.callbacks.LearningRateMonitor>
/n/home10/pafischer/.local/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:166: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python my_experiment.py ...
  rank_zero_warn(
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/n/home10/pafischer/.local/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory checkpoints exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
Restoring states from the checkpoint path at /n/netscratch/mahadevan_lab/Everyone/pafischer/walk-jump-fork/checkpoints/running_checkpoint.ckpt
Error executing job with overrides: []
Traceback (most recent call last):
  File "/n/netscratch/mahadevan_lab/Everyone/pafischer/walk-jump-fork/src/walkjump/cmdline/_train.py", line 41, in train
    trainer.fit(model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "/n/home10/pafischer/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 608, in fit
    call._call_and_handle_interrupt(
  File "/n/home10/pafischer/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 38, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/n/home10/pafischer/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 650, in _fit_impl
    self._run(model, ckpt_path=self.ckpt_path)
  File "/n/home10/pafischer/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1047, in _run
    self._restore_modules_and_callbacks(ckpt_path)
  File "/n/home10/pafischer/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 989, in _restore_modules_and_callbacks
    self._checkpoint_connector.resume_start(checkpoint_path)
  File "/n/home10/pafischer/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/checkpoint_connector.py", line 90, in resume_start
    loaded_checkpoint = self.trainer.strategy.load_checkpoint(checkpoint_path)
  File "/n/home10/pafischer/.local/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 359, in load_checkpoint
    return self.checkpoint_io.load_checkpoint(checkpoint_path)
  File "/n/home10/pafischer/.local/lib/python3.10/site-packages/lightning/fabric/plugins/io/torch_io.py", line 84, in load_checkpoint
    raise FileNotFoundError(f"Checkpoint at {path} not found. Aborting training.")
FileNotFoundError: Checkpoint at /n/netscratch/mahadevan_lab/Everyone/pafischer/walk-jump-fork/checkpoints/running_checkpoint.ckpt not found. Aborting training.
