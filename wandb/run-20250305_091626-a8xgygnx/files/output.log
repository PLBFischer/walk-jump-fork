/n/home10/pafischer/.local/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:166: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python my_experiment.py ...
  rank_zero_warn(
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/n/home10/pafischer/.local/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory checkpoints exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [MIG-de4312ac-8748-5f43-ad1b-cc2814b861fe]
  | Name  | Type        | Params
--------------------------------------
0 | model | ByteNetArch | 1.0 M
--------------------------------------
1.0 M     Trainable params
0         Non-trainable params
1.0 M     Total params
4.104     Total estimated model params size (MB)
/n/home10/pafischer/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/n/home10/pafischer/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/n/home10/pafischer/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:1600: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
  rank_zero_warn(
Epoch 0, global step 1: 'val_loss' reached 0.33333 (best 0.33333), saving model to 'checkpoints/epoch=0-step=1-val_loss=0.3333-v1.ckpt' as top 1
Epoch 1, global step 2: 'val_loss' reached 0.22135 (best 0.22135), saving model to 'checkpoints/epoch=1-step=2-val_loss=0.2214.ckpt' as top 2
[instantiate_callbacks] Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[instantiate_callbacks] Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[instantiate_callbacks] Instantiating callback <lightning.pytorch.callbacks.LearningRateMonitor>
Epoch 1: 100%|██████████| 2/2 [00:00<00:00, 10.60it/s, loss=0.341, v_num=ygnx]
Epoch 2, global step 3: 'val_loss' reached 0.17747 (best 0.17747), saving model to 'checkpoints/epoch=2-step=3-val_loss=0.1775-v1.ckpt' as top 3
Epoch 3, global step 4: 'val_loss' reached 0.15584 (best 0.15584), saving model to 'checkpoints/epoch=3-step=4-val_loss=0.1558-v1.ckpt' as top 4
Epoch 4, global step 5: 'val_loss' reached 0.14186 (best 0.14186), saving model to 'checkpoints/epoch=4-step=5-val_loss=0.1419.ckpt' as top 5
Epoch 6:  50%|█████     | 1/2 [00:00<00:00,  9.21it/s, loss=0.216, v_num=ygnx]
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]
Epoch 6, global step 7: 'val_loss' reached 0.11709 (best 0.11709), saving model to 'checkpoints/epoch=6-step=7-val_loss=0.1171-v1.ckpt' as top 7
Epoch 7, global step 8: 'val_loss' reached 0.10353 (best 0.10353), saving model to 'checkpoints/epoch=7-step=8-val_loss=0.1035-v1.ckpt' as top 8
Epoch 8, global step 9: 'val_loss' reached 0.09273 (best 0.09273), saving model to 'checkpoints/epoch=8-step=9-val_loss=0.0927-v1.ckpt' as top 9
Epoch 9, global step 10: 'val_loss' reached 0.08461 (best 0.08461), saving model to 'checkpoints/epoch=9-step=10-val_loss=0.0846-v2.ckpt' as top 10

Epoch 10, global step 11: 'val_loss' reached 0.07753 (best 0.07753), saving model to 'checkpoints/epoch=10-step=11-val_loss=0.0775-v1.ckpt' as top 11
Epoch 11, global step 12: 'val_loss' reached 0.07303 (best 0.07303), saving model to 'checkpoints/epoch=11-step=12-val_loss=0.0730-v1.ckpt' as top 12
Epoch 12, global step 13: 'val_loss' reached 0.06830 (best 0.06830), saving model to 'checkpoints/epoch=12-step=13-val_loss=0.0683-v5.ckpt' as top 13
Epoch 13, global step 14: 'val_loss' reached 0.06590 (best 0.06590), saving model to 'checkpoints/epoch=13-step=14-val_loss=0.0659-v2.ckpt' as top 14
Epoch 15:  50%|█████     | 1/2 [00:00<00:00,  9.16it/s, loss=0.141, v_num=ygnx]
Validation: 0it [00:00, ?it/s]
Epoch 15, global step 16: 'val_loss' reached 0.06109 (best 0.06109), saving model to 'checkpoints/epoch=15-step=16-val_loss=0.0611-v4.ckpt' as top 16
Epoch 16, global step 17: 'val_loss' reached 0.06071 (best 0.06071), saving model to 'checkpoints/epoch=16-step=17-val_loss=0.0607-v3.ckpt' as top 17
Epoch 17, global step 18: 'val_loss' reached 0.05895 (best 0.05895), saving model to 'checkpoints/epoch=17-step=18-val_loss=0.0590-v2.ckpt' as top 18
Epoch 18, global step 19: 'val_loss' reached 0.05805 (best 0.05805), saving model to 'checkpoints/epoch=18-step=19-val_loss=0.0580-v6.ckpt' as top 19
Epoch 19, global step 20: 'val_loss' reached 0.05713 (best 0.05713), saving model to 'checkpoints/epoch=19-step=20-val_loss=0.0571.ckpt' as top 20

`Trainer.fit` stopped: `max_epochs=20` reached.